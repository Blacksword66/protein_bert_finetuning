{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model for any of the benchmarks by editing the BENCHMAKR_NAME variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14945 training set records, 1661 validation set records, 4152 test set records.\n",
      "       label                                                seq\n",
      "13682      1  MEAPAQLLFLLLLWLPDTTGEIVMTQSPATLSVSPGERATLSCRAS...\n",
      "14810      0  MAFISTPLGKVTVKSATVSANRRGLRMQSDSEPVVSRRALLSGALA...\n",
      "5059       1  MARSKTAQPKHSLRKIAVVVATAVSGMSVYAQAAVEPKEDTITVTA...\n",
      "6566       0  MSLLRIRKPKTRKGKKVLLAREPQLIESARTMLFLDGRKCGGNVKL...\n",
      "10033      0  MWNSGFESYGSSSYGGAGGYTQSPGGFGSPAPSQAEKKSRARAQHI...\n",
      "3300       0  MRKRISAIIMTLFMVLASCSNQLEAEKLAAESKNTFFDSLVKIGQG...\n",
      "9804       0  MAPVVQQPAPSFKKTAVVDGVFEEVTLEQYKGKWVLLAFIPLAFTF...\n",
      "3958       0  MAPKSDNTEAIVLNFVNEQNKPLNTQNAADALQKFNLKKTAVQKAL...\n",
      "2713       0  MSYYQRPFSPSAYSLPASLNSSIVMQHGTSLDSTDTYPQHAQSLDG...\n",
      "10760      0  MASSSNTMEFEEDDSTVTQTSLPTTTSVLDKSRSLFGNSAVISTPA...\n",
      "[2024_07_23-15:47:14] Training set: Filtered out 0 of 14945 (0.0%) records of lengths exceeding 510.\n",
      "[2024_07_23-15:47:15] Validation set: Filtered out 0 of 1661 (0.0%) records of lengths exceeding 510.\n",
      "[2024_07_23-15:47:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 15:47:15.400690: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2024-07-23 15:47:15.400714: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-07-23 15:47:15.400721: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-07-23 15:47:15.401289: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-23 15:47:15.401564: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-07-23 15:47:20.519097: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575/575 [==============================] - ETA: 0s - loss: 0.4538WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "2024-07-23 15:48:54.412062: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575/575 [==============================] - 110s 182ms/step - loss: 0.4538 - val_loss: 0.4501 - lr: 2.0000e-04\n",
      "[2024_07_23-15:49:07] Training the entire fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-07-23 15:49:12.865468: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024_07_23-15:49:21] Incompatible number of optimizer weights - will not initialize them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 15:49:24.509390: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575/575 [==============================] - ETA: 0s - loss: 0.2670WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "2024-07-23 15:53:18.894580: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575/575 [==============================] - 254s 420ms/step - loss: 0.2670 - val_loss: 0.2523 - lr: 2.0000e-04\n",
      "[2024_07_23-15:53:35] Training on final epochs of sequence length 1024...\n",
      "[2024_07_23-15:53:35] Training set: Filtered out 0 of 14945 (0.0%) records of lengths exceeding 1022.\n",
      "[2024_07_23-15:53:36] Validation set: Filtered out 0 of 1661 (0.0%) records of lengths exceeding 1022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-07-23 15:53:49.352043: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-07-23 15:54:01.520622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150/1150 [==============================] - ETA: 0s - loss: 0.2430WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "2024-07-23 16:02:25.871110: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150/1150 [==============================] - 533s 454ms/step - loss: 0.2430 - val_loss: 0.2321 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-07-23 16:03:03.614259: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 32s 175ms/step\n",
      "seq_len = 512\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>4152</td>\n",
       "      <td>0.501837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4152</td>\n",
       "      <td>0.501837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                 4152  0.501837\n",
       "All                 4152  0.501837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  3478  0\n",
       "1   674  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Benchmark name and output specification\n",
    "BENCHMARK_NAME = 'signalP_binary'\n",
    "BENCHMARKS_DIR = '/Users/yeyatiprasher/Coding/Internship/protein_bert/protein_benchmarks'\n",
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
    "\n",
    "# Directory containing benchmarks\n",
    "\n",
    "# Loading the dataset\n",
    "train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % BENCHMARK_NAME)\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "train_set, valid_set = train_test_split(train_set, stratify=train_set['label'], test_size=0.1, random_state=100)\n",
    "\n",
    "test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % BENCHMARK_NAME)\n",
    "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "\n",
    "print(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')\n",
    "print(valid_set.head(10))\n",
    "\n",
    "# Loading the pre-trained model and fine-tuning it on the loaded dataset\n",
    "batch_size = 45\n",
    "seq_len = 512\n",
    "final = seq_len * 2\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "# get_model_with_hidden_layers_as_outputs gives the model output access to the hidden layers (on top of the output)\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function=get_model_with_hidden_layers_as_outputs, dropout_rate=0.17681099042260753)\n",
    "\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.25, min_lr=1e-05, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'],\n",
    "         seq_len=seq_len, batch_size=batch_size, max_epochs_per_stage=1, lr=8.741510119145999e-05, begin_with_frozen_pretrained_layers=True,\n",
    "         lr_with_frozen_pretrained_layers=1e-02, n_final_epochs=1, final_seq_len=final, final_lr=1e-05, callbacks=training_callbacks)\n",
    "\n",
    "    # Evaluating the performance on the test set\n",
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_set['seq'], test_set['label'],\n",
    "                                            start_seq_len=seq_len, start_batch_size=batch_size)\n",
    "print(\"seq_len = %d\" % seq_len)\n",
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding number of uniquely labeled sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "BENCHMARKS_DIR = '/Users/yeyatiprasher/Coding/Internship/protein_bert/protein_benchmarks'\n",
    "BENCHMARK_NAME = 'ProFET_NP_SP_Cleaved'\n",
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0,1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
    "\n",
    "train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % BENCHMARK_NAME)\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "#train_set, valid_set = train_test_split(train_set, stratify=train_set['label'], test_size=0.1, random_state=100)\n",
    "\n",
    "test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % BENCHMARK_NAME)\n",
    "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "\n",
    "\n",
    "print(\"Training Set pos : \", len(train_set[train_set[\"label\"] == 1]))\n",
    "print(\"Training Set neg : \", len(train_set[train_set[\"label\"] == 0]))\n",
    "print(len(train_set))\n",
    "print(\"Testing Set pos : \", len(test_set[test_set[\"label\"] == 1]))\n",
    "print(\"Testing Set neg : \", len(test_set[test_set[\"label\"] == 0]))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "BENCHMARKS_DIR = '/Users/yeyatiprasher/Coding/Internship/protein_bert/protein_benchmarks'\n",
    "BENCHMARK_NAME = 'scop'\n",
    "OUTPUT_TYPE = OutputType(False, 'categorical')\n",
    "UNIQUE_LABELS = ['a','b','c','d','e','f','g']\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
    "\n",
    "train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % BENCHMARK_NAME)\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "train_set, valid_set = train_test_split(train_set, stratify=train_set['label'], test_size=0.1, random_state=100)\n",
    "\n",
    "test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % BENCHMARK_NAME)\n",
    "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "\n",
    "\n",
    "print(\"Training Set a : \", len(train_set[train_set[\"label\"] == 'a']))\n",
    "print(\"Training Set b : \", len(train_set[train_set[\"label\"] == 'b']))\n",
    "print(\"Training Set c : \", len(train_set[train_set[\"label\"] == 'c']))\n",
    "print(\"Training Set d : \", len(train_set[train_set[\"label\"] == 'd']))\n",
    "print(\"Training Set e : \", len(train_set[train_set[\"label\"] == 'e']))\n",
    "print(\"Training Set f : \", len(train_set[train_set[\"label\"] == 'f']))\n",
    "print(\"Training Set g : \", len(train_set[train_set[\"label\"] == 'g']))\n",
    "print(len(train_set))\n",
    "print(\"Valid Set a : \", len(valid_set[valid_set[\"label\"] == 'a']))\n",
    "print(\"Valid Set b  : \", len(valid_set[valid_set[\"label\"] == 'b']))\n",
    "print(\"Valid Set c : \", len(valid_set[valid_set[\"label\"] == 'c']))\n",
    "print(\"Valid Set d : \", len(valid_set[valid_set[\"label\"] == 'd']))\n",
    "print(\"Valid Set e : \", len(valid_set[valid_set[\"label\"] == 'e']))\n",
    "print(\"Valid Set f : \", len(valid_set[valid_set[\"label\"] == 'f']))\n",
    "print(\"Valid Set g : \", len(valid_set[valid_set[\"label\"] == 'g']))\n",
    "print(len(valid_set))\n",
    "print(\"Testing Set a : \", len(test_set[test_set[\"label\"] == 'a']))\n",
    "print(\"Testing Set b : \", len(test_set[test_set[\"label\"] == 'b']))\n",
    "print(\"Testing Set c : \", len(test_set[test_set[\"label\"] == 'c']))\n",
    "print(\"Testing Set d : \", len(test_set[test_set[\"label\"] == 'd']))\n",
    "print(\"Testing Set e : \", len(test_set[test_set[\"label\"] == 'e']))\n",
    "print(\"Testing Set f : \", len(test_set[test_set[\"label\"] == 'f']))\n",
    "print(\"Testing Set g : \", len(test_set[test_set[\"label\"] == 'g']))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import tokenization as tk\n",
    "import pandas as pd\n",
    "import csv\n",
    "train_set_file_path = \"/Users/yeyatiprasher/Coding/Internship/protein_bert/protein_benchmarks/ProFET_NP_SP_Cleaved.train.csv\"\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "f = open(\"/Users/yeyatiprasher/Coding/Internship/protein_bert/NPCleaved_embed.csv\",\"w\")\n",
    "writer = csv.writer(f)\n",
    "for i in train_set['seq']:\n",
    "    embed = tk.tokenize_seq(i)\n",
    "    writer.writerow(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import tokenization as tk\n",
    "import pandas as pd\n",
    "import csv\n",
    "train_set_file_path = \"/Users/yeyatiprasher/Coding/Internship/protein_bert/protein_benchmarks/scop.train.csv\"\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "f = open(\"/Users/yeyatiprasher/Coding/Internship/protein_bert/scop_embed.csv\",\"w\")\n",
    "writer = csv.writer(f)\n",
    "for i in train_set['seq']:\n",
    "    embed = tk.tokenize_seq(i)\n",
    "    writer.writerow(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import optuna\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "BENCHMARK_NAME = 'ProFET_NP_SP_Cleaved'\n",
    "BENCHMARKS_DIR = '/Users/yeyatiprasher/Coding/Internship/protein_bert/protein_benchmarks'\n",
    "\n",
    "# A local (non-global) binary output\n",
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    #batch_size = trial.suggest_int('batch_size', 20, 64)\n",
    "    seed = trial.suggest_int('seed',80,110)\n",
    "    #lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    #dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    # Loading the dataset\n",
    "    train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % BENCHMARK_NAME)\n",
    "    train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "    train_set, valid_set = train_test_split(train_set, stratify=train_set['label'], test_size=0.1, random_state=seed)\n",
    "\n",
    "    test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % BENCHMARK_NAME)\n",
    "    test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "    print('%d training set records, %d validation set records, %d test set records.' % (len(train_set), len(valid_set), len(test_set)))\n",
    "\n",
    "    pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "    model_generator = FinetuningModelGenerator(\n",
    "        pretrained_model_generator, OUTPUT_SPEC, \n",
    "        pretraining_model_manipulation_function=get_model_with_hidden_layers_as_outputs,\n",
    "        dropout_rate=0.17681099042260753\n",
    "    )\n",
    "\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.25, min_lr=1e-05, verbose=1),\n",
    "        keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "    ]\n",
    "\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \n",
    "             seq_len=512, batch_size=32, max_epochs_per_stage=1, lr=8.741510119145999e-05, \n",
    "             begin_with_frozen_pretrained_layers=True, lr_with_frozen_pretrained_layers=1e-02, \n",
    "             n_final_epochs=1, final_seq_len=1024, final_lr=1e-05, callbacks=training_callbacks)\n",
    "\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_set['seq'], test_set['label'], \n",
    "                                                start_seq_len=512, start_batch_size=32)\n",
    "    \n",
    "    overall_accuracy = results.loc['All', 'AUC']\n",
    "    print(overall_accuracy)\n",
    "    return overall_accuracy\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "print('Best accuracy: ', study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
